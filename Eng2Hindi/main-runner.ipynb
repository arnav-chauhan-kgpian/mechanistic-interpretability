{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14144871,"sourceType":"datasetVersion","datasetId":9014546}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchmetrics tokenizers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile config.py\nfrom pathlib import Path\n\ndef get_config():\n    return {\n        \"batch_size\": 64,\n        \"num_epochs\": 20,\n        \"lr\": 2e-4,\n        \"seq_len\": 128,\n        \"d_model\": 512, # Changed from 360 to 512 (Standard practice: 512/8 = 64 per head)\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"hi\", # CHANGED: Tamil (ta) -> Hindi (hi)\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\", # CHANGED: Just the prefix, not the full path\n        \"preload\": None, # Set to 'latest' to resume training if interrupted\n        \"tokenizer_file\": \"tokenizer_{0}.json\",          \n        \"experiment_name\": \"runs/tmodel\",\n        \"N\": 6,\n        \"h\": 8,\n        \"dropout\": 0.1 # Reduced slightly (0.2 is okay, but 0.1 is standard for this size)\n    }\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder']\n    model_basename = config['model_basename']\n    model_filename = f\"{model_basename}{epoch}.pt\"\n    return str(Path('.') / model_folder / model_filename)\n\ndef latest_weights_file_path(config):\n    # CHANGED: Removed dependency on missing 'datasource' key\n    model_folder = config['model_folder']\n    model_basename = config['model_basename']\n    \n    # Check if folder exists\n    if not Path(model_folder).exists():\n        return None\n        \n    model_filename = f\"{model_basename}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    \n    if len(weights_files) == 0:\n        return None\n    \n    weights_files.sort()\n    return str(weights_files[-1])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile dataset.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\n\nclass BilingualDataset(Dataset):\n\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n\n        # Store special token IDs directly as integers (more efficient)\n        self.sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')], dtype=torch.int64)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, index):\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair[self.src_lang]\n        tgt_text = src_target_pair[self.tgt_lang]\n\n        # Splitting sentences into tokens\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        # Truncate tokens if too long\n        # Encoder: needs space for SOS, EOS\n        if len(enc_input_tokens) > self.seq_len - 2:\n            enc_input_tokens = enc_input_tokens[:self.seq_len - 2]\n        # Decoder: needs space for SOS\n        if len(dec_input_tokens) > self.seq_len - 1:\n            dec_input_tokens = dec_input_tokens[:self.seq_len - 1]\n\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n\n        # Create padding tensors efficiently\n        # We use .item() to get the int value, then create the tensor\n        enc_padding = torch.full((enc_num_padding_tokens,), self.pad_token.item(), dtype=torch.int64)\n        dec_padding = torch.full((dec_num_padding_tokens,), self.pad_token.item(), dtype=torch.int64)\n\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                enc_padding\n            ],\n            dim=0,\n        )\n\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                dec_padding\n            ],\n            dim=0\n        )\n\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                dec_padding\n            ],\n            dim=0,\n        )\n\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len)\n            \"label\": label, # (seq_len)\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text\n        }\n\ndef causal_mask(size):\n    # Defines the upper triangular matrix (including diag) as 0\n    # Returns 1 for lower triangle (keep), 0 for upper (ignore)\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile model.py\nimport torch\nimport torch.nn as nn\nimport math\n\nclass InputEmbeddings(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embeddings = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        # (batch, seq_len) -> (batch, seq_len, d_model)\n        # Multiply by sqrt(d_model) to scale embeddings (as per paper)\n        return self.embeddings(x) * math.sqrt(self.d_model)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_length: int, dropout: float):\n        super().__init__()\n        self.d_model = d_model\n        self.seq_length = seq_length\n        self.dropout = nn.Dropout(dropout)\n\n        # Create a matrix of shape (seq_length, d_model)\n        pe = torch.zeros(seq_length, d_model)\n        \n        # Create a vector of shape (seq_length, 1) for positions\n        position = torch.arange(0, seq_length, dtype=torch.float32).unsqueeze(1)\n        \n        # Calculate the division term\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # Apply sine to even indices and cosine to odd indices\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # Add batch dimension: (1, seq_length, d_model)\n        pe = pe.unsqueeze(0)\n\n        # Register as a buffer (not a learnable parameter, but part of state_dict)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add positional encoding to embeddings (slicing to current seq length)\n        x = x + self.pe[:, :x.shape[1], :]\n        return self.dropout(x)\n\nclass LayerNormalisation(nn.Module):\n    def __init__(self, eps: float = 10**-6):\n        super().__init__()\n        self.eps = eps\n        # Learnable parameters\n        self.alpha = nn.Parameter(torch.ones(1)) # Multiplicative\n        self.bias = nn.Parameter(torch.zeros(1)) # Additive\n\n    def forward(self, x):\n        # Mean and Std calculated over the last dimension\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.alpha * ((x - mean) / (std + self.eps)) + self.bias\n\nclass FeedForwardBlock(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout: float):\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff) # W1 + b1\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model) # W2 + b2\n\n    def forward(self, x):\n        # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, d_ff) -> (Batch, Seq_Len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n\nclass MultiHeadAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, h: int, dropout: float):\n        super().__init__()\n        self.d_model = d_model\n        self.h = h\n        \n        assert d_model % h == 0, \"d_model must be divisible by h\"\n\n        self.d_k = d_model // h\n        \n        self.w_q = nn.Linear(d_model, d_model) # Wq\n        self.w_k = nn.Linear(d_model, d_model) # Wk\n        self.w_v = nn.Linear(d_model, d_model) # Wv\n        self.w_o = nn.Linear(d_model, d_model) # Wo\n        \n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n        \n        # (Batch, h, Seq_Len, d_k) @ (Batch, h, d_k, Seq_Len) -> (Batch, h, Seq_Len, Seq_Len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        if mask is not None:\n            # Apply mask (0 means ignore, so we set to -infinity)\n            attention_scores.masked_fill_(mask == 0, -1e9)\n            \n        attention_scores = attention_scores.softmax(dim=-1)\n        \n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n            \n        # (Batch, h, Seq_Len, Seq_Len) @ (Batch, h, Seq_Len, d_k) -> (Batch, h, Seq_Len, d_k)\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask):\n        query = self.w_q(q) # (Batch, Seq_Len, d_model)\n        key = self.w_k(k)   # (Batch, Seq_Len, d_model)\n        value = self.w_v(v) # (Batch, Seq_Len, d_model)\n\n        # Reshape for multi-head attention:\n        # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, h, d_k) -> (Batch, h, Seq_Len, d_k)\n        query = query.view(query.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n\n        # Calculate attention\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n        \n        # Combine heads\n        # (Batch, h, Seq_Len, d_k) -> (Batch, Seq_Len, h, d_k) -> (Batch, Seq_Len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n        \n        # Output projection\n        return self.w_o(x)\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, dropout: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalisation()\n\n    def forward(self, x, sublayer):\n        # \"Norm first\" pre-LN architecture is often more stable, but original paper used Post-LN.\n        # Your implementation is Post-LN (Norm after Sublayer + Add): \n        # x + dropout(sublayer(norm(x))) -> This is actually Pre-LN structure (Norm is applied to input of sublayer)\n        # This is generally BETTER for training stability.\n        return x + self.dropout(sublayer(self.norm(x)))\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalisation()\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, target_mask):\n        # 1. Self Attention (with target mask to prevent look-ahead)\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n        # 2. Cross Attention (Query=Decoder, Key/Value=Encoder)\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        # 3. Feed Forward\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalisation()\n\n    def forward(self, x, encoder_output, src_mask, target_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, target_mask)\n        return self.norm(x)\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        # (Batch, Seq_Len, d_model) -> (Batch, Seq_Len, vocab_size)\n        # CRITICAL FIX: Returning RAW LOGITS.\n        # Use nn.CrossEntropyLoss during training (which applies Softmax internally).\n        return self.proj(x)\n\nclass Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, target_embed: InputEmbeddings, src_pos: PositionalEncoding, target_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.target_embed = target_embed\n        self.src_pos = src_pos\n        self.target_pos = target_pos\n        self.projection_layer = projection_layer\n\n    def encode(self, src, src_mask):\n        src = self.src_embed(src)\n        src = self.src_pos(src)\n        return self.encoder(src, src_mask)\n\n    def decode(self, encoder_output, src_mask, target, target_mask):\n        target = self.target_embed(target)\n        target = self.target_pos(target)\n        return self.decoder(target, encoder_output, src_mask, target_mask)\n\n    def project(self, x):\n        return self.projection_layer(x)\n\ndef build_transformer(src_vocab_size: int, target_vocab_size: int, src_seq_len: int, target_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048):\n    \n    # Create Embedding Layers\n    src_embed = InputEmbeddings(src_vocab_size, d_model)\n    target_embed = InputEmbeddings(target_vocab_size, d_model)\n\n    # Create Positional Encoding Layers\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    target_pos = PositionalEncoding(d_model, target_seq_len, dropout)\n\n    # Create Encoder Blocks\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    # Create Decoder Blocks\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n\n    # Assemble Encoder and Decoder\n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n\n    # Create Projection Layer\n    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n\n    # Create the complete Transformer\n    transformer = Transformer(encoder, decoder, src_embed, target_embed, src_pos, target_pos, projection_layer)\n\n    # Initialize parameters with Xavier Uniform\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n\n    return transformer","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\n\nimport warnings\nfrom pathlib import Path\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport torchmetrics\n\n# Huggingface imports\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\n# CHANGED: Added Punctuation and Sequence imports\nfrom tokenizers.pre_tokenizers import Whitespace, Punctuation, Sequence\n\n# Local imports\nfrom model import build_transformer\nfrom dataset import BilingualDataset, causal_mask\nfrom config import get_config, get_weights_file_path, latest_weights_file_path\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n    \n    # Precompute encoder output and reuse it for every token we get from the decoder\n    encoder_output = model.encode(source, source_mask)\n    # Initialise the decoder input with the sos token\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n    \n    while True:\n        if decoder_input.size(1) == max_len :\n            break\n        \n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n        \n        # Calculate the decoder output\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n        \n        # Get the next token\n        prob = model.project(out[:,-1])\n        \n        # Select the token with the maximum probability\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n\n        if next_word == eos_idx:\n            break\n            \n    return decoder_input.squeeze(0)\n\ndef run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples = 2):\n    model.eval()\n    count = 0\n\n    source_texts = []\n    expected = []\n    predicted = []\n\n    try:\n        import shutil\n        console_width = shutil.get_terminal_size().columns\n    except:\n        console_width = 80\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n            source_texts.append(source_text)\n            expected.append(target_text)\n            predicted.append(model_out_text)\n            \n            print_msg('-'*console_width)\n            print_msg(f\"{'SOURCE: ':>12}{source_text}\")\n            print_msg(f\"{'TARGET: ':>12}{target_text}\")\n            print_msg(f\"{'PREDICTED: ':>12}{model_out_text}\")\n\n            if count == num_examples:\n                print_msg('-'*console_width)\n                break\n    \n    if writer:\n        # TorchMetrics expects lists of strings\n        metric = torchmetrics.CharErrorRate()\n        cer = metric(predicted, expected)\n        writer.add_scalar('validation cer', cer, global_state)\n        writer.flush()\n\n        metric = torchmetrics.WordErrorRate()\n        wer = metric(predicted, expected)\n        writer.add_scalar('validation wer', wer, global_state)\n        writer.flush()\n\n        metric = torchmetrics.BLEUScore()\n        bleu = metric(predicted, expected)\n        writer.add_scalar('validation BLEU', bleu, global_state)\n        writer.flush()\n\ndef get_all_sentences(ds, lang):\n    for item in ds:\n        if 'translation' in item:\n            yield item['translation'][lang]\n        else:\n            yield item[lang]\n\ndef get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not tokenizer_path.exists():\n        print(f\"Tokenizer file {tokenizer_path} not found. Training a new one...\")\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        \n        # CHANGED: Updated to use Sequence(Whitespace, Punctuation)\n        # This matches the structure of your uploaded JSON files.\n        tokenizer.pre_tokenizer = Sequence([Whitespace(), Punctuation(behavior=\"Isolated\")])\n        \n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n\ndef get_ds(config):\n    print(\"Loading Hindi Dataset...\")\n    ds_raw = load_dataset('cfilt/iitb-english-hindi', split='train[:100000]') \n    \n    flattened_ds = []\n    for item in ds_raw:\n        if 'translation' in item:\n            flattened_ds.append(item['translation'])\n        else:\n            flattened_ds.append(item)\n            \n    ds_raw = flattened_ds\n\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    train_ds_size = int(0.9 * len(ds_raw))\n    train_ds_raw = ds_raw[:train_ds_size]\n    val_ds_raw = ds_raw[train_ds_size:]\n\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    max_len_src = 0\n    max_len_tgt = 0\n\n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item[config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item[config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of the source sentence : {max_len_src}')\n    print(f'Max length of the target sentence : {max_len_tgt}')\n\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model\n\ndef train_model(config):\n    # Setup Device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f'Using device {device}')\n    \n    # Create model folder\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n    \n    # Log file path\n    log_file_path = Path(config['model_folder']) / \"validation_logs.txt\"\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n    writer = SummaryWriter(log_dir=config['experiment_name'])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n\n    initial_epoch = 0\n    global_step = 0\n    preload = config['preload']\n    \n    if preload == 'latest':\n        model_filename = latest_weights_file_path(config)\n    elif preload:\n        model_filename = get_weights_file_path(config, preload)\n    else:\n        model_filename = None\n\n    if model_filename:\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename, map_location=device)\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, config['num_epochs']):\n        torch.cuda.empty_cache()\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n        \n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            label = batch['label'].to(device)\n\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            \n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            global_step += 1\n\n        # Custom print function for logging\n        def print_msg(msg):\n            batch_iterator.write(msg)\n            with open(log_file_path, \"a\", encoding='utf-8') as f:\n                f.write(msg + \"\\n\")\n\n        print_msg(f\"\\n--- Epoch {epoch:02d} Validation ---\")\n        \n        # FIXED LINE: Changed 'global_state' to 'global_step'\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, print_msg, global_step, writer)\n\n        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n        \nif __name__ == '__main__':\n    warnings.filterwarnings('ignore')\n    config = get_config()\n    train_model(config)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile translate.py\nfrom pathlib import Path\nimport torch\nimport sys\nfrom config import get_config, latest_weights_file_path\nfrom model import build_transformer\nfrom tokenizers import Tokenizer\nfrom dataset import causal_mask\n\ndef translate(sentence: str):\n    # 1. Load Config and Device\n    config = get_config()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # 2. Load Tokenizers\n    # We load the existing files (we do NOT retrain them)\n    tokenizer_src = Tokenizer.from_file(config['tokenizer_file'].format(config['lang_src']))\n    tokenizer_tgt = Tokenizer.from_file(config['tokenizer_file'].format(config['lang_tgt']))\n\n    # 3. Build Model\n    model = build_transformer(\n        tokenizer_src.get_vocab_size(), \n        tokenizer_tgt.get_vocab_size(), \n        config['seq_len'], \n        config['seq_len'], \n        d_model=config['d_model']\n    ).to(device)\n\n    # 4. Load Pre-trained Weights\n    model_filename = latest_weights_file_path(config)\n    if not model_filename:\n        print(\"No weights found! Train the model first.\")\n        return\n    \n    print(f\"Loading weights from: {model_filename}\")\n    state = torch.load(model_filename, map_location=device)\n    model.load_state_dict(state['model_state_dict'])\n    model.eval() # Switch to evaluation mode\n\n    # 5. Prepare Input Text\n    # Encode the sentence and add SOS/EOS\n    sos_token = tokenizer_tgt.token_to_id('[SOS]')\n    eos_token = tokenizer_tgt.token_to_id('[EOS]')\n    \n    encoder_input_tokens = tokenizer_src.encode(sentence).ids\n    encoder_input = torch.tensor(\n        [sos_token] + encoder_input_tokens + [eos_token], \n        dtype=torch.int64\n    ).to(device)\n    \n    # Add batch dimension (1, seq_len)\n    encoder_input = encoder_input.unsqueeze(0) \n    \n    # Create Encoder Mask (1, 1, 1, seq_len)\n    # Since we are doing inference on 1 sentence, we don't strictly need padding mask if we handle lengths right,\n    # but strictly speaking, we should mask padding if we had it. Here we have no padding.\n    encoder_mask = (encoder_input != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n\n    # 6. Run Encoder\n    with torch.no_grad():\n        encoder_output = model.encode(encoder_input, encoder_mask)\n\n        # 7. Auto-regressive Decoder (Greedy Decode)\n        # Start with just [SOS]\n        decoder_input = torch.empty(1, 1).fill_(sos_token).type_as(encoder_input).to(device)\n\n        while True:\n            if decoder_input.size(1) == config['seq_len']:\n                break\n\n            # Create Mask for Decoder\n            decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n\n            # Calculate output\n            out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n\n            # Get next token probability\n            prob = model.project(out[:, -1])\n            _, next_word = torch.max(prob, dim=1)\n\n            # Append next word to decoder input\n            decoder_input = torch.cat(\n                [decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], \n                dim=1\n            )\n\n            if next_word == eos_token:\n                break\n\n    # 8. Decode the result back to text\n    # Squeeze batch dim, remove SOS\n    output_ids = decoder_input.squeeze(0).tolist()\n    \n    translated_text = tokenizer_tgt.decode(output_ids)\n    print(f\"\\nEnglish: {sentence}\")\n    print(f\"Hindi:   {translated_text}\")\n\nif __name__ == '__main__':\n    # Usage: python translate.py \"Hello world\"\n    if len(sys.argv) > 1:\n        sentence = sys.argv[1]\n        translate(sentence)\n    else:\n        print(\"Please provide a sentence. Example: python translate.py 'Hello world'\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copy tokenizers from Input (Read-only) to Working (Read-Write)\n!cp /kaggle/input/toktok/tokenizer_en.json .\n!cp /kaggle/input/toktok/tokenizer_hi.json .","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python translate.py \"I am a student of this institute.\"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}